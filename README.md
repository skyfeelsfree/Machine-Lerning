
---

````markdown
# ğŸ’‡â€â™€ï¸ Hair Fall Level Prediction using Machine Learning & Deep Learning

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ì–‘í•œ ê¸°ê³„í•™ìŠµê³¼ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ **ë¨¸ë¦¬ì¹´ë½ ë¹ ì§(hair fall)** ì •ë„ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤. Google Driveì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , ì „ì²˜ë¦¬, ëª¨ë¸ í•™ìŠµ, ì‹œê°í™”ë¥¼ ëª¨ë‘ í¬í•¨í•œ **êµìœ¡ìš© ì¢…í•© ì‹¤ìŠµ** ì½”ë“œì…ë‹ˆë‹¤.

## âœ… ì‚¬ìš©í•œ ê¸°ìˆ 
- ë°ì´í„° ì „ì²˜ë¦¬: pandas, sklearn
- ì‹œê°í™”: matplotlib
- ëª¨ë¸: ë‹¤ì¤‘ ì„ í˜• íšŒê·€, ëœë¤ í¬ë ˆìŠ¤íŠ¸, ë”¥ëŸ¬ë‹(Keras)
- í‰ê°€ ì§€í‘œ: MSE, MAE, RÂ² Score

---

## 1. ğŸ“ êµ¬ê¸€ ë§ˆìš´íŠ¸ ë° ë°ì´í„° ìˆ˜ì§‘

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
url = "https://drive.google.com/uc?id=1UriMxGMrB960jzfMDRpSxGA20cL0Vz7E"
df = pd.read_csv(url)

# ë°ì´í„° í™•ì¸
print(df.info())
print(df.head())
````

---

## 2. ğŸ§¼ ì „ì²˜ë¦¬

```python
# ê²°ì¸¡ê°’ í‰ê· ìœ¼ë¡œ ì±„ìš°ê¸°
df.fillna(df.mean(), inplace=True)

# ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ (One-Hot Encoding)
encoder = OneHotEncoder(sparse_output=False)
hair_texture_encoded = encoder.fit_transform(df[['hair_texture']])
df = df.drop(columns=['hair_texture'])
df = pd.concat([df, pd.DataFrame(hair_texture_encoded)], axis=1)

# ì»¬ëŸ¼ ì´ë¦„ ë¬¸ìì—´ë¡œ ë³€í™˜
df.columns = df.columns.astype(str)

# ë°ì´í„° ì •ê·œí™”
scaler = MinMaxScaler()
df[df.columns] = scaler.fit_transform(df)

# ì…ë ¥/ì¶œë ¥ ë¶„ë¦¬
X = df.drop(columns=['hair_fall'])
Y = df['hair_fall']
```

---

## 3. ğŸ¤– ê¸°ê³„í•™ìŠµ ëª¨ë¸ ì ìš©í•˜ê¸°

```python
# ì…ë ¥/ì¶œë ¥ ë¶„ë¦¬
X = df.drop(columns=['hair_fall'])
Y = df['hair_fall']

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ë‹¤ì¤‘ ì„ í˜• íšŒê·€
lin_reg = LinearRegression()
lin_reg.fit(X_train, Y_train)
Y_pred = lin_reg.predict(X_test)
print(f"Linear Regression MSE: {mean_squared_error(Y_test, Y_pred):.4f}")
print(f"Linear Regression MAE: {mean_absolute_error(Y_test, Y_pred):.4f}")
print(f"Linear Regression RÂ² Score: {r2_score(Y_test, Y_pred):.4f}")

# ëœë¤ í¬ë ˆìŠ¤íŠ¸
rf_reg = RandomForestRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)
rf_reg.fit(X_train, Y_train)
Y_pred_rf = rf_reg.predict(X_test)
print(f"Random Forest MSE: {mean_squared_error(Y_test, Y_pred_rf):.4f}")
print(f"Random Forest MAE: {mean_absolute_error(Y_test, Y_pred_rf):.4f}")
print(f"Random Forest RÂ² Score: {r2_score(Y_test, Y_pred_rf):.4f}")
```

---

## 4. ğŸ§  ë”¥ëŸ¬ë‹ ëª¨ë¸ ì¶”ê°€

```python
from keras.models import Sequential
from keras.layers import Dense
import tensorflow as tf

# ì‹œë“œ ê³ ì •
tf.random.set_seed(3)

# ëª¨ë¸ êµ¬ì„±
model = Sequential()
model.add(Dense(28, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(7, activation='relu'))
model.add(Dense(1))

# ì»´íŒŒì¼
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])

# í•™ìŠµ
history = model.fit(X_train, Y_train, epochs=15, batch_size=10, validation_split=0.25)

# ì‹œê°í™”
plt.plot(history.history['loss'], 'b-', label='Train Loss')
plt.plot(history.history['val_loss'], 'r--', label='Validation Loss')
plt.xlabel('Epoch')
plt.legend()
plt.title('Loss Progress')
plt.show()

# í‰ê°€
loss, mse = model.evaluate(X_test, Y_test)
print(f"Deep Learning Model - MSE: {mse:.4f}")
```

---

## 5. ğŸ“Š ì‹œê°í™” ë° ëª¨ë¸ ë¹„êµ

```python
# ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ
plt.figure(figsize=(10, 5))
plt.scatter(Y_test, Y_pred, label='Linear Regression', alpha=0.5, color='blue')
plt.scatter(Y_test, Y_pred_rf, label='Random Forest', alpha=0.5, color='green')
plt.scatter(Y_test, model.predict(X_test), label='Deep Learning', alpha=0.5, color='red')
plt.xlabel("Actual Hair Fall Level")
plt.ylabel("Predicted Hair Fall Level")
plt.legend()
plt.title("Comparison of Predictions from Different Models")
plt.show()

# ì˜¤ì°¨ ë¶„í¬
residuals_lin = Y_test - Y_pred
residuals_rf = Y_test - Y_pred_rf
residuals_dl = Y_test - model.predict(X_test).flatten()

plt.figure(figsize=(10, 5))
plt.hist(residuals_lin, bins=30, alpha=0.5, label='Linear Regression', color='blue')
plt.hist(residuals_rf, bins=30, alpha=0.5, label='Random Forest', color='green')
plt.hist(residuals_dl, bins=30, alpha=0.5, label='Deep Learning', color='red')
plt.xlabel("Prediction Error")
plt.ylabel("Frequency")
plt.title("Residual Distribution of Different Models")
plt.legend()
plt.show()

# ì„±ëŠ¥ ë¹„êµ ì§€í‘œ ì‹œê°í™”
metrics = {
    "MSE": [mean_squared_error(Y_test, Y_pred), mean_squared_error(Y_test, Y_pred_rf), mse],
    "MAE": [mean_absolute_error(Y_test, Y_pred), mean_absolute_error(Y_test, Y_pred_rf), mean_absolute_error(Y_test, model.predict(X_test).flatten())],
    "RÂ² Score": [r2_score(Y_test, Y_pred), r2_score(Y_test, Y_pred_rf), None]
}

df_metrics = pd.DataFrame(metrics, index=['Linear Regression', 'Random Forest', 'Deep Learning'])
df_metrics.plot(kind='bar', figsize=(10, 5))
plt.title("Model Performance Comparison")
plt.ylabel("Score")
plt.xticks(rotation=0)
plt.show()
```

---

## ğŸ“Œ ì°¸ê³  ì‚¬í•­

* ì´ ì½”ë“œëŠ” êµìœ¡ìš© ì˜ˆì œë¡œ, ì‹¤ì œ ë°ì´í„° ë¶„ì„ í”„ë¡œì íŠ¸ì—ì„œëŠ” ê³¼ì í•© ë°©ì§€, ë” ì •êµí•œ ì „ì²˜ë¦¬, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë“±ì˜ ê³¼ì •ì´ ì¶”ê°€ë¡œ í•„ìš”í•©ë‹ˆë‹¤.
* ë°ì´í„° ì¶œì²˜ëŠ” Google Drive ë§í¬ë¡œ ì œê³µë©ë‹ˆë‹¤.
* ë”¥ëŸ¬ë‹ì˜ RÂ² ScoreëŠ” ê¸°ë³¸ ì œê³µë˜ì§€ ì•Šìœ¼ë¯€ë¡œ, í•„ìš”í•˜ë©´ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## âœ¨ ëª©ì 

ì´ í”„ë¡œì íŠ¸ëŠ” **ê¸°ì´ˆì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ë° ë”¥ëŸ¬ë‹ ì‹¤ìŠµì„ í†µí•´ ì „ì²˜ë¦¬ë¶€í„° ì˜ˆì¸¡, ì‹œê°í™”ê¹Œì§€ ì „ ê³¼ì •ì„ ê²½í—˜**í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ëª¨ë¸ì„ ë¹„êµí•˜ê³  í•™ìŠµ ì„±ëŠ¥ì„ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.

```

--- 
